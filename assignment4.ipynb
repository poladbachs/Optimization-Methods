{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b17e8fa7",
   "metadata": {},
   "source": [
    "### Part 2: Programming Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ff48ca",
   "metadata": {},
   "source": [
    "#### Problem 1 – Line Search\n",
    "##### (1) Define the function $f(x)$\n",
    "\n",
    "We consider the function\n",
    "$$\n",
    "f(x)= x_1^2+2x_2^2,\n",
    "$$\n",
    "where $x=(x_1,x_2)\\in\\mathbb{R}^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e1c4c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f_line(x):\n",
    "    # x is a numpy array: [x1, x2]\n",
    "    x1, x2 = x[0], x[1]\n",
    "    return x1**2 + 2*x2**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1c4cdd",
   "metadata": {},
   "source": [
    "##### (2) Define the gradient of $f(x)$\n",
    "\n",
    "The gradient is\n",
    "$$\n",
    "\\nabla f(x)= \\begin{pmatrix} 2x_1 \\\\ 4x_2 \\end{pmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f74ac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_f_line(x):\n",
    "    x1, x2 = x[0], x[1]\n",
    "    return np.array([2*x1, 4*x2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a51bf4",
   "metadata": {},
   "source": [
    "##### (3) Evaluate $f(x)$ and $\\nabla f(x)$ at $x^{(0)}=(9,1)^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5632fad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x0) = 83\n",
      "grad f(x0) = [18  4]\n"
     ]
    }
   ],
   "source": [
    "x0_line = np.array([9, 1])\n",
    "print(\"f(x0) =\", f_line(x0_line))\n",
    "print(\"grad f(x0) =\", grad_f_line(x0_line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b13a2c",
   "metadata": {},
   "source": [
    "##### (4) Define the function $wolfe\\_conditions$\n",
    "\n",
    "This function verifies if the Wolfe conditions hold for a given step size $\\alpha>0$. It takes:\n",
    "- A function $f$\n",
    "- Its gradient function $\\nabla f$\n",
    "- A current point $x$\n",
    "- A descent direction $d$\n",
    "- A step size $\\alpha$\n",
    "- Parameters $\\eta$ and $\\bar{\\eta}$\n",
    "\n",
    "and returns a tuple of booleans $(\\text{first\\_wolfe}, \\text{second\\_wolfe})$.\n",
    "\n",
    "The Wolfe conditions are:\n",
    "1. First Wolfe condition (sufficient decrease):\n",
    "$$\n",
    "f(x+\\alpha d)\\le f(x)+\\alpha\\eta\\,\\nabla f(x)^T d.\n",
    "$$\n",
    "2. Second Wolfe condition (curvature):\n",
    "$$\n",
    "\\nabla f(x+\\alpha d)^T d \\ge \\bar{\\eta}\\,\\nabla f(x)^T d.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf6f00da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wolfe_conditions(f, grad_f, x, d, alpha, eta, eta_bar):\n",
    "    cond1 = f(x + alpha * d) <= f(x) + alpha * eta * np.dot(grad_f(x), d)\n",
    "    cond2 = np.dot(grad_f(x + alpha * d), d) >= eta_bar * np.dot(grad_f(x), d)\n",
    "    return (cond1, cond2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acf138a",
   "metadata": {},
   "source": [
    "##### (5) Test the $wolfe\\_conditions$ function\n",
    "\n",
    "Test for $f(x)=x_1^2+2x_2^2$ at $x^{(0)}=(9,1)^T$ with \n",
    "$$\n",
    "d^{(0)}=-\\nabla f(x^{(0)}),\n",
    "$$ \n",
    "$\\alpha=0.05$, $\\eta=0.01$, and $\\bar{\\eta}=0.8$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e739b5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wolfe conditions (first, second): (True, False)\n"
     ]
    }
   ],
   "source": [
    "d0_line = -grad_f_line(x0_line)\n",
    "wolfe_test = wolfe_conditions(f_line, grad_f_line, x0_line, d0_line, 0.05, 0.01, 0.8)\n",
    "print(\"Wolfe conditions (first, second):\", wolfe_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72339250",
   "metadata": {},
   "source": [
    "##### (6) Define the function $backtracking$\n",
    "\n",
    "Implement the backtracking line search. It takes:\n",
    "- A function $f$\n",
    "- Its gradient $\\nabla f$\n",
    "- A current point $x$\n",
    "- A descent direction $d$\n",
    "- A maximum step size $\\bar{\\alpha}$\n",
    "- Parameter $\\eta$ (for the first Wolfe condition)\n",
    "\n",
    "and returns the step size $\\alpha^*$ and the new point $x_{\\text{new}} = x+\\alpha^* d$.  \n",
    "The function should use the $wolfe\\_conditions$ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9a2b5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtracking(f, grad_f, x, d, alpha_bar, eta, eta_bar=0.8, tau=0.5):\n",
    "    alpha = alpha_bar\n",
    "    while True:\n",
    "        cond1, cond2 = wolfe_conditions(f, grad_f, x, d, alpha, eta, eta_bar)\n",
    "        if cond1 and cond2:\n",
    "            break\n",
    "        alpha *= tau\n",
    "        if alpha < 1e-8:\n",
    "            break\n",
    "    x_new = x + alpha * d\n",
    "    return alpha, x_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a689f844",
   "metadata": {},
   "source": [
    "##### (7) Test the Backtracking Function\n",
    "\n",
    "Test the backtracking function for $f(x)=x_1^2+2x_2^2$ at $x^{(0)}=(9,1)^T$ with \n",
    "$$\n",
    "d^{(0)}=-\\nabla f(x^{(0)}),\n",
    "$$ \n",
    "$\\bar{\\alpha}=10$, and $\\eta=0.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9323bc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Backtracking: α* = 0.625\n",
      "Test Backtracking: x_new = [-2.25 -1.5 ]\n"
     ]
    }
   ],
   "source": [
    "alpha_star, x_new_line = backtracking(f_line, grad_f_line, x0_line, d0_line, alpha_bar=10, eta=0.01)\n",
    "print(\"Test Backtracking: α* =\", alpha_star)\n",
    "print(\"Test Backtracking: x_new =\", x_new_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ec122e",
   "metadata": {},
   "source": [
    "#### Problem 2 – Unconstrained Optimisation\n",
    "##### (1) Define the function\n",
    "\n",
    "We consider:\n",
    "$$\n",
    "f(x)= x_1^3 - x_1 + x_2^3 - x_2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e5131f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_uncon(x):\n",
    "    # x is a numpy array [x1, x2]\n",
    "    x1, x2 = x[0], x[1]\n",
    "    return x1**3 - x1 + x2**3 - x2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e290ab36",
   "metadata": {},
   "source": [
    "##### (2) Define the gradient of $f(x)$\n",
    "\n",
    "For $x_1^3-x_1$, the derivative is $3x_1^2-1$.  \n",
    "For $x_2^3-x_2$, the derivative is $3x_2^2-1$.  \n",
    "\n",
    "Thus,\n",
    "$$\n",
    "\\nabla f(x)= \\begin{pmatrix} 3x_1^2-1 \\\\ 3x_2^2-1 \\end{pmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f728964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_f_uncon(x):\n",
    "    # Compute gradient components for f(x)=x1^3-x1+x2^3-x2\n",
    "    x1, x2 = x[0], x[1]\n",
    "    return np.array([3*x1**2 - 1, 3*x2**2 - 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3246bbe",
   "metadata": {},
   "source": [
    "##### (3) Fixed-step Gradient Descent\n",
    "\n",
    "We implement gradient descent with the update rule:\n",
    "$$\n",
    "x^{(k+1)} = x^{(k)} - \\alpha\\,\\nabla f(x^{(k)}),\n",
    "$$\n",
    "with a constant step size $\\alpha$.  \n",
    "We use the starting point $x^{(0)}=(1,1)^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb983663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed-step GD final x: [0.5870474 0.5870474]\n",
      "Fixed-step GD f(x): -0.7694727904830598\n"
     ]
    }
   ],
   "source": [
    "def fixed_gd(f, grad_f, x_start, alpha, max_iter=1000, tol=1e-6):\n",
    "    # Initialize current point and store iterates\n",
    "    x_current = np.array(x_start, dtype=float)\n",
    "    iterates = [x_current.copy()]\n",
    "    for _ in range(max_iter):\n",
    "        grad = grad_f(x_current)\n",
    "        # Stop if the gradient norm is below tolerance\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            break\n",
    "        # Update rule for gradient descent\n",
    "        x_current = x_current - alpha * grad\n",
    "        iterates.append(x_current.copy())\n",
    "    return x_current, iterates\n",
    "\n",
    "x0_uncon = np.array([1, 1])\n",
    "alpha_fixed = 0.001  # Chosen fixed step size\n",
    "x_fixed, iterates_fixed = fixed_gd(f_uncon, grad_f_uncon, x0_uncon, alpha_fixed)\n",
    "print(\"Fixed-step GD final x:\", x_fixed)\n",
    "print(\"Fixed-step GD f(x):\", f_uncon(x_fixed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7a2845",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
